{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "PCM   +---------+        +---------+ PCM\n",
    "----->| Encoder |------->| Decoder |----->\n",
    "audio +---------+ stream +---------+ audio'\n",
    "\n",
    "              audio != audio'\n",
    "                (usually)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical encoder steps\n",
    "\n",
    "1. **Overlaped subband analysis** (usually with [MDCT](http://en.wikipedia.org/wiki/Modified_discrete_cosine_transform} (Modified\n",
    "   Discrete Cosine Transform)). Goes from the temporal to a frequency\n",
    "   domain.\n",
    "  \n",
    "2. **Quantization**. Basically, removes pure signals of low amplitude\n",
    "   but taking also into account the SAM (pSycho Acoustic Model) of the\n",
    "   HAS (Human Auditory System). Noise use to be of low power!\n",
    "   \n",
    "3. [**Entropy coding**](https://github.com/vicente-gonzalez-ruiz/teaching/blob/master/coding/text/text_coding.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lossy coding\n",
    "\n",
    "* The limitations of human perception are incorporated into the compression process through the use of psychoacoustic models. Some of these limitations are physiological, based on the machinery of hearing. Others are psychological, based on how our brain processes auditory stimuli.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlaped processing\n",
    "\n",
    "```\n",
    "0              N-1            2N-1            3N-1\n",
    "+---------------+---------------+---------------+ s[n]\n",
    "<--------Transform Step--------->\n",
    "                <---------Transform Step-------->\n",
    "```\n",
    "\n",
    "* Each transform step inputs $2N$ samples and outputs $N$ MDCT\n",
    "  coeficients.\n",
    "  \n",
    "* $N$ can vary depending on the characteristics of the sound. For\n",
    "  \\emph{complex} sounds without clear armonics (such as a plosive sound),\n",
    "  shortened windows improve the performance. For \\emph{simple} sounds\n",
    "  (such as a music instrument), large windows are better.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency resolution and simultaneous masking\n",
    "\n",
    "* The HAS has a limited frequency resolution. Psychoacoustic\n",
    "  experiments have demonstrated that the audible frequencies can be\n",
    "  grouped into \\href{../../../Perception/Auditive_perception/index.html#x1-50004}{barks}.\n",
    "\n",
    "* Each bark defines the group of frequencies that excite the same\n",
    "  cochlear area, i.e., those frequencies that can be masked by the\n",
    "  tone with the highest energy (in that bark).\n",
    "\n",
    "* In the cochlea, a frequency-to-place transformation\n",
    "takes place which leads to the notion of critical bands\n",
    "Critical bandwidth can be considered as the\n",
    "bandwidth at which subjective responses change\n",
    "abruptly. For example, the perceived loudness of a\n",
    "narrow band noise at constant sound pressure level\n",
    "remains constant within a critical band and then,\n",
    "begins to increase, once the bandwidth of the stimulus\n",
    "is increased beyond one critical band [5] (Zwicker E., Fastl H, “Psychoacoustics, Facts\n",
    "and Models” Springer Verlag, 1990 ).\n",
    "\n",
    "Bark Scale | Lower edge | Upper edge | Bandwidth | Center Frequency\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal masking\n",
    "\n",
    "* The human auditory system has inertia:\n",
    "  \\href{../../../Perception/Auditive_perception/index.html#x1-70006}{sounds\n",
    "    are not instantly perceived and remains after they are disapered}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel coupling\n",
    "\n",
    "* Most of the time, similar sounds are transported in the channels\n",
    "  of a non-mono audio signal. Channel coupling decreases inter-channel\n",
    "  redundancy, usually, using prediction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "* Depending on the desired output bit-rate and the frequency (see\n",
    "  the ATH model), the SAM applies a different quantization step to\n",
    "  barks (see Section~\\ref{sec:ATH}). Roughly speaking, the higher the\n",
    "  compression ratio, the larger the quantization step and therefore,\n",
    "  the quantization noise; and the higher the frequency, the wider the\n",
    "  bark. Notice also that the perception of a tone in a bark depends\n",
    "  also on the temporal masking.\n",
    "\n",
    "* At decoding time, those barks that suffered the biggest lossess\n",
    "  are usually filled with [white noise](http://en.wikipedia.org/wiki/White_noise) in\n",
    "  order to [ncrease the perceived quality](http://simplynoise.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Coding\n",
    "\n",
    "* Usually, a variable bit-rate (VBR) lossless encoding algorithm\n",
    "  asigns code-words of less bits to those code-vectors (one or more\n",
    "  quantized MDCT coefficients) with a high probability, and viceversa,\n",
    "  producing an effective reduction of the bit-rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDCT\n",
    "\n",
    "* Equivalent to apply a [bank of $N$ filters](http://en.wikipedia.org/wiki/Filter_bank).\n",
    "\n",
    "* Determines the correlation between a set of $2N$ numbers\n",
    "  (samples) and $N$\n",
    "  [orthogonal](http://en.wikipedia.org/wiki/Orthogonality) (two\n",
    "  functions/signals are orthogonal if it is impossible to obtain one\n",
    "  of them by means of the other.) [cosine functions](http://guru.multimedia.cx/mdct/). \n",
    "  Therefore, at the input of the DCT there are $2N$ samples and at the output,\n",
    "  $N$ coefficients.\n",
    "  \n",
    "* MDCT coefficients $S[w]$ of the PCM samples $s[n]$ are\n",
    "  defined as:\n",
    "  \\begin{equation}\n",
    "    S[w] = \\sum_{n=0}^{2N-1}s[n]cos\\Big[\\frac{\\pi}{N}(n+\\frac{1}{2}+\\frac{N}{2})(w+\\frac{1}{2})\\Big].\n",
    "    \\label{eq:MDCT}\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Delta Modulation](delta_modulation/DM.ipynb).\n",
    "1. [The sound](The_Sound/the_sound.ipynb).\n",
    "2. [The human auditory system](The_Human_Auditory_System/the_human_auditory_system.ipynb).\n",
    "3. [Human sound perception](Human_Sound_Perception/human_sound_perception.ipynb).\n",
    "6. [FLAC](FLAC/FLAC.ipynb).\n",
    "8. [MPEG Audio Layer 3 (MP3)](MP3/MP3.ipynb).\n",
    "9. [MPEG Advanced Audio Coding (AAC)](AAC/AAC.ipynb).\n",
    "10. [Vorbis](Vorbis/Vorbis.ipynb).\n",
    "11. [Dolby Digital (AC3)](Dolby_Digital_AC3/AC3.ipynb).\n",
    "12. [DTS](DTS/DTS.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
